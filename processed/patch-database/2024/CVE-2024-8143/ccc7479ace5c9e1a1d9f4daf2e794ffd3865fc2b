diff --git a/ChuanhuChatbot.py b/ChuanhuChatbot.py
index 0bae4348..90101e72 100644
--- a/ChuanhuChatbot.py
+++ b/ChuanhuChatbot.py
@@ -87,12 +87,6 @@ def create_new_model():
                             with gr.Column(min_width=42, scale=1):
                                 historyDeleteBtn = gr.Button(
                                     "🗑️", elem_id="gr-history-delete-btn")
-                            with gr.Column(min_width=42, scale=1):
-                                historyDownloadBtn = gr.Button(
-                                    "⏬", elem_id="gr-history-download-btn")
-                            with gr.Column(min_width=42, scale=1):
-                                historyMarkdownDownloadBtn = gr.Button(
-                                    "⤵️", elem_id="gr-history-mardown-download-btn")
                     with gr.Row(visible=False):
                         with gr.Column(scale=6):
                             saveFileName = gr.Textbox(
@@ -126,6 +120,9 @@ def create_new_model():
                         label=i18n("选择模型"), choices=[], multiselect=False, interactive=True, visible=False,
                         container=False,
                     )
+                    # with gr.Column(min_width=150, scale=1, elem_id="chatbot-header-btn-bar"):
+                    downloadHistoryJSONBtn = gr.DownloadButton(i18n("历史记录（JSON）"))
+                    downloadHistoryMarkdownBtn = gr.DownloadButton(i18n("导出为 Markdown"))
                     gr.HTML(get_html("chatbot_header_btn.html").format(
                         json_label=i18n("历史记录（JSON）"),
                         md_label=i18n("导出为 Markdown")
@@ -519,10 +516,10 @@ def create_greeting(request: gr.Request):
             loaded_stuff = current_model.auto_load()
         else:
             current_model.new_auto_history_filename()
-            loaded_stuff = [gr.update(), gr.update(), gr.Chatbot(label=MODELS[DEFAULT_MODEL]), current_model.single_turn, current_model.temperature, current_model.top_p, current_model.n_choices, current_model.stop_sequence, current_model.token_upper_limit, current_model.max_generation_token, current_model.presence_penalty, current_model.frequency_penalty, current_model.logit_bias, current_model.user_identifier]
+            loaded_stuff = [gr.update(), gr.update(), gr.Chatbot(label=MODELS[DEFAULT_MODEL]), current_model.single_turn, current_model.temperature, current_model.top_p, current_model.n_choices, current_model.stop_sequence, current_model.token_upper_limit, current_model.max_generation_token, current_model.presence_penalty, current_model.frequency_penalty, current_model.logit_bias, current_model.user_identifier, gr.DownloadButton(), gr.DownloadButton()]
         return user_info, user_name, current_model, toggle_like_btn_visibility(DEFAULT_MODEL), *loaded_stuff, init_history_list(user_name, prepend=current_model.history_file_path.rstrip(".json"))
     demo.load(create_greeting, inputs=None, outputs=[
-              user_info, user_name, current_model, like_dislike_area, saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox, historySelectList], api_name="load")
+              user_info, user_name, current_model, like_dislike_area, saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox, downloadHistoryJSONBtn, downloadHistoryMarkdownBtn, historySelectList], api_name="load")
     chatgpt_predict_args = dict(
         fn=predict,
         inputs=[
@@ -566,7 +563,7 @@ def create_greeting(request: gr.Request):
     load_history_from_file_args = dict(
         fn=load_chat_history,
         inputs=[current_model, historySelectList],
-        outputs=[saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox],
+        outputs=[saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox, downloadHistoryJSONBtn, downloadHistoryMarkdownBtn],
     )
 
     refresh_history_args = dict(
@@ -709,11 +706,7 @@ def create_greeting(request: gr.Request):
     )
     historySelectList.select(**load_history_from_file_args)
     uploadHistoryBtn.upload(upload_chat_history, [current_model, uploadHistoryBtn], [
-                        saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox, historySelectList]).then(**refresh_history_args)
-    historyDownloadBtn.click(None, [
-                             user_name, historySelectList], None, js='(a,b)=>{return downloadHistory(a,b,".json");}')
-    historyMarkdownDownloadBtn.click(None, [
-                                     user_name, historySelectList], None, js='(a,b)=>{return downloadHistory(a,b,".md");}')
+                        saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox, downloadHistoryJSONBtn, downloadHistoryMarkdownBtn, historySelectList]).then(**refresh_history_args)
     historySearchTextbox.input(
         filter_history,
         [user_name, historySearchTextbox],
@@ -807,7 +800,7 @@ def create_greeting(request: gr.Request):
     historySelectBtn.click(  # This is an experimental feature... Not actually used.
         fn=load_chat_history,
         inputs=[current_model, historySelectList],
-        outputs=[saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox],
+        outputs=[saveFileName, systemPromptTxt, chatbot, single_turn_checkbox, temperature_slider, top_p_slider, n_choices_slider, stop_sequence_txt, max_context_length_slider, max_generation_slider, presence_penalty_slider, frequency_penalty_slider, logit_bias_txt, user_identifier_txt, use_streaming_checkbox, downloadHistoryJSONBtn, downloadHistoryMarkdownBtn],
         js='(a,b)=>{return bgSelectHistory(a,b);}'
     )
 # 默认开启本地服务器，默认可以直接从IP访问，默认不创建公开分享链接
@@ -817,8 +810,8 @@ def create_greeting(request: gr.Request):
     reload_javascript()
     setup_wizard()
     demo.queue().launch(
-        allowed_paths=["history", "web_assets"],
-        blocked_paths=["config.json", "files", "models", "lora", "modules"],
+        allowed_paths=["web_assets"],
+        blocked_paths=["config.json", "files", "models", "lora", "modules", "history"],
         server_name=server_name,
         server_port=server_port,
         share=share,
diff --git a/modules/models/base_model.py b/modules/models/base_model.py
index 29de40d9..d60e12c5 100644
--- a/modules/models/base_model.py
+++ b/modules/models/base_model.py
@@ -17,6 +17,8 @@
 from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, TypeVar, Union
 from uuid import UUID
 from langchain_core.outputs import ChatGenerationChunk, GenerationChunk
+from gradio.utils import get_upload_folder
+from gradio.processing_utils import save_file_to_cache
 
 import colorama
 import PIL
@@ -34,6 +36,7 @@
 from ..presets import *
 from ..utils import *
 
+GRADIO_CACHE = get_upload_folder()
 
 class CallbackToIterator:
     def __init__(self):
@@ -1005,13 +1008,14 @@ def upload_chat_history(self, new_history_file_content=None):
                     with open(new_history_file_path, 'w', encoding='utf-8') as f:
                         json.dump(json_content, f, ensure_ascii=False, indent=2)
 
-                    self.history_file_path = new_history_filename
-                    logging.info(f"History file uploaded and saved as {new_history_filename}")
+                    self.history_file_path = new_history_filename[:-5]
+                    save_md_file(os.path.join(HISTORY_DIR, self.user_name, new_history_filename))
+                    logging.info(f"History file uploaded and saved as {self.history_file_path}")
                 except json.JSONDecodeError:
                     logging.error("Uploaded content is not valid JSON. Using default history.")
             else:
                 logging.warning("Unexpected type for new_history_file_content. Using default history.")
-        return *self.load_chat_history(new_history_file_path), init_history_list(self.user_name)
+        return *self.load_chat_history(), init_history_list(self.user_name)
 
     def load_chat_history(self, new_history_file_path=None):
         logging.debug(f"{self.user_name} 加载对话历史中……")
@@ -1074,6 +1078,11 @@ def load_chat_history(self, new_history_file_path=None):
             self.metadata = saved_json.get("metadata", self.metadata)
             self.stream = saved_json.get("stream", self.stream)
             self.chatbot = saved_json["chatbot"]
+
+            history_json_path = os.path.realpath(os.path.join(HISTORY_DIR, self.user_name, self.history_file_path + ".json"))
+            history_md_path = os.path.realpath(os.path.join(HISTORY_DIR, self.user_name, self.history_file_path + ".md"))
+            tmp_json_for_download = save_file_to_cache(history_json_path, GRADIO_CACHE)
+            tmp_md_for_download = save_file_to_cache(history_md_path, GRADIO_CACHE)
             return (
                 os.path.basename(self.history_file_path)[:-5],
                 saved_json["system"],
@@ -1089,7 +1098,9 @@ def load_chat_history(self, new_history_file_path=None):
                 self.frequency_penalty,
                 self.logit_bias,
                 self.user_identifier,
-                self.stream
+                self.stream,
+                gr.DownloadButton(value=tmp_json_for_download, interactive=True),
+                gr.DownloadButton(value=tmp_md_for_download, interactive=True),
             )
         except:
             # 没有对话历史或者对话历史解析失败
@@ -1110,7 +1121,9 @@ def load_chat_history(self, new_history_file_path=None):
                 self.frequency_penalty,
                 self.logit_bias,
                 self.user_identifier,
-                self.stream
+                self.stream,
+                gr.DownloadButton(value=None, interactive=False),
+                gr.DownloadButton(value=None, interactive=False),
             )
 
     def delete_chat_history(self, filename):
diff --git a/modules/utils.py b/modules/utils.py
index e6b12b01..a74d092a 100644
--- a/modules/utils.py
+++ b/modules/utils.py
@@ -454,17 +454,20 @@ def save_file(filename, model):
     with open(history_file_path, "w", encoding="utf-8") as f:
         json.dump(json_s, f, ensure_ascii=False, indent=4)
 
-    filename = os.path.basename(filename)
-    filename_md = filename[:-5] + ".md"
-    md_s = f"system: \n- {system} \n"
-    for data in history:
+    save_md_file(history_file_path)
+    return history_file_path
+
+def save_md_file(json_file_path):
+    with open(json_file_path, "r", encoding="utf-8") as f:
+        json_data = json.load(f)
+
+    md_file_path = json_file_path[:-5] + ".md"
+    md_s = f"system: \n- {json_data['system']} \n"
+    for data in json_data['history']:
         md_s += f"\n{data['role']}: \n- {data['content']} \n"
-    with open(
-        os.path.join(HISTORY_DIR, user_name, filename_md), "w", encoding="utf8"
-    ) as f:
-        f.write(md_s)
-    return os.path.join(HISTORY_DIR, user_name, filename)
 
+    with open(md_file_path, "w", encoding="utf8") as f:
+        f.write(md_s)
 
 def sorted_by_pinyin(list):
     return sorted(list, key=lambda char: lazy_pinyin(char)[0][0])
@@ -1543,3 +1546,6 @@ def setPlaceholder(model_name: str | None = "", model: BaseLLMModel | None = Non
             chatbot_ph_slogan_class = slogan_class,
             chatbot_ph_question_class = question_class
         )
+
+def download_file(path):
+    print(path)
