From bippy-d175d3acf727 Mon Sep 17 00:00:00 2001
From: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
To: <linux-cve-announce@vger.kernel.org>
Reply-to: <cve@kernel.org>, <linux-kernel@vger.kernel.org>
Subject: CVE-2024-27005: interconnect: Don't access req_list while it's being manipulated

Description
===========

In the Linux kernel, the following vulnerability has been resolved:

interconnect: Don't access req_list while it's being manipulated

The icc_lock mutex was split into separate icc_lock and icc_bw_lock
mutexes in [1] to avoid lockdep splats. However, this didn't adequately
protect access to icc_node::req_list.

The icc_set_bw() function will eventually iterate over req_list while
only holding icc_bw_lock, but req_list can be modified while only
holding icc_lock. This causes races between icc_set_bw(), of_icc_get(),
and icc_put().

Example A:

  CPU0                               CPU1
  ----                               ----
  icc_set_bw(path_a)
    mutex_lock(&icc_bw_lock);
                                     icc_put(path_b)
                                       mutex_lock(&icc_lock);
    aggregate_requests()
      hlist_for_each_entry(r, ...
                                       hlist_del(...
        <r = invalid pointer>

Example B:

  CPU0                               CPU1
  ----                               ----
  icc_set_bw(path_a)
    mutex_lock(&icc_bw_lock);
                                     path_b = of_icc_get()
                                       of_icc_get_by_index()
                                         mutex_lock(&icc_lock);
                                         path_find()
                                           path_init()
    aggregate_requests()
      hlist_for_each_entry(r, ...
                                             hlist_add_head(...
        <r = invalid pointer>

Fix this by ensuring icc_bw_lock is always held before manipulating
icc_node::req_list. The additional places icc_bw_lock is held don't
perform any memory allocations, so we should still be safe from the
original lockdep splats that motivated the separate locks.

[1] commit af42269c3523 ("interconnect: Fix locking for runpm vs reclaim")

The Linux kernel CVE team has assigned CVE-2024-27005 to this issue.


Affected and fixed versions
===========================

	Issue introduced in 6.6 with commit af42269c3523 and fixed in 6.6.29 with commit d0d04efa2e36
	Issue introduced in 6.6 with commit af42269c3523 and fixed in 6.8.8 with commit 4c65507121ea
	Issue introduced in 6.6 with commit af42269c3523 and fixed in 6.9 with commit de1bf25b6d77
	Issue introduced in 5.15.133 with commit 9be2957f014d
	Issue introduced in 5.15.151 with commit fe549d8e9763
	Issue introduced in 6.1.55 with commit ee42bfc791aa
	Issue introduced in 6.1.81 with commit 19ec82b3cad1
	Issue introduced in 6.5.5 with commit 2f3a124696d4

Please see https://www.kernel.org for a full list of currently supported
kernel versions by the kernel community.

Unaffected versions might change over time as fixes are backported to
older supported kernel versions.  The official CVE entry at
	https://cve.org/CVERecord/?id=CVE-2024-27005
will be updated if fixes are backported, please check that for the most
up to date information about this issue.


Affected files
==============

The file(s) affected by this issue are:
	drivers/interconnect/core.c


Mitigation
==========

The Linux kernel CVE team recommends that you update to the latest
stable kernel version for this, and many other bugfixes.  Individual
changes are never tested alone, but rather are part of a larger kernel
release.  Cherry-picking individual commits is not recommended or
supported by the Linux kernel community at all.  If however, updating to
the latest release is impossible, the individual changes to resolve this
issue can be found at these commits:
	https://git.kernel.org/stable/c/d0d04efa2e367921654b5106cc5c05e3757c2b42
	https://git.kernel.org/stable/c/4c65507121ea8e0b47fae6d2049c8688390d46b6
	https://git.kernel.org/stable/c/de1bf25b6d771abdb52d43546cf57ad775fb68a1
